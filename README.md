# Info
  - I referred to [paul-hyun's Code](https://paul-hyun.github.io/bert-01/) for understanding BERT model as a newbie in NLP domain
  - And, I'm googling to complete my codebase for training BERT with GLUE BENCHMARK
# About BERT
 - As described in [BERT paper](https://arxiv.org/pdf/1810.04805.pdf), BERT should be trained in two steps.
 - First, pretrain a BERT model with two tasks denoted by masked language model and next sentence prediction.
 - Second, fine-tune the pretrained BERT model for each task. 
   - Since I just want to check the benchmark score for some of my other task, in this repository, I only provide the related code base
    and information.
     
# Requirements
 - pytorch 1.7+, numpy, python 3.6+, sentencepiece, tqdm

# Usage
 - To be updated